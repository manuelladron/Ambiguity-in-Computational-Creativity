{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp2 = spacy.load('en_core_web_sm')\n",
    "merge_ents = nlp2.create_pipe(\"merge_entities\")\n",
    "nlp2.add_pipe(merge_ents)\n",
    "#all_stop_words = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "furniture_path = \"../json_files/cleaned/furniture_cleaned.json\"\n",
    "fashion_path = \"../json_files/cleaned/fashion_cleaned.json\"\n",
    "wearable_tech = \"../json_files/cleaned/wearable_tech_cleaned.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outpaths \n",
    "furniture_tokenized = \"../json_files/tokenized/furniture_tokenized.json\"\n",
    "furniture_contexts = \"../json_files/context_words/furniture_context\"\n",
    "\n",
    "fashion_tokenized = \"../json_files/tokenized/fashion_tokenized.json\"\n",
    "fashion_contexts = \"../json_files/context_words/fashion_context\"\n",
    "\n",
    "wearable_tokenized = \"../json_files/tokenized/wearable_tech_tokenized.json\"\n",
    "wearable_contexts = \"../json_files/context_words/wearable_tech_context\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json(path):\n",
    "    f = open(path) \n",
    "    data = json.load(f) \n",
    "    f.close()\n",
    "    return data \n",
    "\n",
    "def save_json(file_path, data):\n",
    "    out_file = open(file_path, \"w\")\n",
    "    json.dump(data, out_file)\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text):\n",
    "    \"\"\"\n",
    "    Returns a list of a dictionary per word in the text.\n",
    "    The dictionary contains the word, its tag and its POS. \n",
    "    \"\"\"\n",
    "    return [{'word': token.text, 'tag': token.tag_, 'pos': token.pos_} for token in nlp2(text) if not token.tag_ == '_SP']\n",
    "\n",
    "def tokenize_json(json_file):\n",
    "    new_data = []\n",
    "    for i, article in enumerate(json_file):\n",
    "        if i != len(json_file)-1:\n",
    "            new_data.append(word_tokenize(article['text']))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_(l):\n",
    "    res = []\n",
    "    for elem in l:\n",
    "        if isinstance(elem, list):\n",
    "            for val in elem:\n",
    "                res.append(val)\n",
    "        else:\n",
    "            res.append(elem)\n",
    "    return res\n",
    "\n",
    "def create_contexts(dataset, window_size, outpath_tokenization, outpath_context, save_tok=True):\n",
    "    \"\"\"\n",
    "    Takes in the processed dataset containing {word, tag, POS}\n",
    "    Returns a list with all context of window_size of adjectives \n",
    "    \"\"\"\n",
    "    all_contexts = []\n",
    "    for article in dataset:                           # iterates over all articles \n",
    "        for i, word in enumerate(article):            # iterates over all words in each article\n",
    "            if word['pos'] == 'ADJ':                  # selects each adj \n",
    "                context = []                          # list with context \n",
    "                if i <= window_size: \n",
    "                    context.append(article[:i])\n",
    "                else:\n",
    "                    context.append(article[i-window_size:i])\n",
    "                \n",
    "                context.append((word, 'root'))\n",
    "                \n",
    "                if i+window_size >= len(article) - 1:\n",
    "                    context.append(article[i+1:])\n",
    "                else:\n",
    "                    context.append(article[i+1:window_size+i+1])\n",
    "                    \n",
    "                context = flatten_(context)\n",
    "                all_contexts.append(context)\n",
    "    if save_tok:\n",
    "        save_json(outpath_tokenization, dataset)\n",
    "    save_json(outpath_context, all_contexts)\n",
    "\n",
    "def run(path, out_token, out_context):\n",
    "    # 1) Open file\n",
    "    json_file = open_json(path)\n",
    "    \n",
    "    # 2) Tokenize file \n",
    "    tokenized_file = tokenize_json(json_file)\n",
    "    \n",
    "    # 3) Get contexts and save \n",
    "    window_sizes = [2,3,5,7,9]\n",
    "    for i, size in enumerate(window_sizes):\n",
    "        if i==0:\n",
    "            save_tok=True\n",
    "        else:\n",
    "            save_tok=False\n",
    "        context_path = out_context + \"_{}.json\".format(size)\n",
    "        create_contexts(tokenized_file, size, out_token, context_path, save_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(furniture_path, furniture_tokenized, furniture_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "furniture_contexts = \"../json_files/context_words/furniture_context_9.json\"\n",
    "a = open_json(furniture_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'the', 'tag': 'DT', 'pos': 'DET'},\n",
       " {'word': 'VDF', 'tag': 'NNP', 'pos': 'PROPN'},\n",
       " {'word': 'x', 'tag': 'SYM', 'pos': 'SYM'},\n",
       " {'word': 'Sight', 'tag': 'NNP', 'pos': 'PROPN'},\n",
       " {'word': 'Unseen', 'tag': 'NNP', 'pos': 'PROPN'},\n",
       " {'word': 'collaboration', 'tag': 'NN', 'pos': 'NOUN'},\n",
       " {'word': ',', 'tag': ',', 'pos': 'PUNCT'},\n",
       " {'word': 'which', 'tag': 'WDT', 'pos': 'DET'},\n",
       " {'word': 'balances', 'tag': 'VBZ', 'pos': 'VERB'},\n",
       " [{'word': 'angular', 'tag': 'JJ', 'pos': 'ADJ'}, 'root'],\n",
       " {'word': ',', 'tag': ',', 'pos': 'PUNCT'},\n",
       " {'word': 'architectural', 'tag': 'JJ', 'pos': 'ADJ'},\n",
       " {'word': 'shapes', 'tag': 'NNS', 'pos': 'NOUN'},\n",
       " {'word': 'with', 'tag': 'IN', 'pos': 'ADP'},\n",
       " {'word': 'playful', 'tag': 'JJ', 'pos': 'ADJ'},\n",
       " {'word': ',', 'tag': ',', 'pos': 'PUNCT'},\n",
       " {'word': 'feminine', 'tag': 'JJ', 'pos': 'ADJ'},\n",
       " {'word': 'touches', 'tag': 'NNS', 'pos': 'NOUN'},\n",
       " {'word': '.', 'tag': '.', 'pos': 'PUNCT'}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
