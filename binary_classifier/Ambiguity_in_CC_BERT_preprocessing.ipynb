{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install python-Levenshtein\n",
    "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
    "!pip install wget\n",
    "%cd ctcdecode\n",
    "!pip install .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.6.0-cp37-cp37m-macosx_10_10_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import * # for pad_sequence and whatnot\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "\n",
    "\"\"\"\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.models import BPE \n",
    "from tokenizers.normalizers import Lowercase, NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "\"\"\"\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To have more information about what's happening under the hood\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
    "!pip install wget\n",
    "%cd ctcdecode\n",
    "!pip install .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/manuelladron/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "INFO:transformers.configuration_utils:Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/manuelladron/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/manuelladron/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = 'abcdefghijklmnopqrstuvwxyz'\n",
    "NUMBERS = '0123456789'\n",
    "class PreprocessedData_wordlevel(object):\n",
    "    def __init__(self, train_file_path, test_file_path):\n",
    "        \"\"\"\n",
    "        train_file_path = list with files\n",
    "        test_file_path = list with files\n",
    "        \"\"\"\n",
    "        \n",
    "        self.train_path = train_file_path\n",
    "        self.test_path = test_file_path\n",
    "        \n",
    "        self.VOCAB = None\n",
    "        self.VOCAB_SIZE = None\n",
    "                \n",
    "        # Automatically run it when making an instance\n",
    "        self.RUN_for_dataset()\n",
    "\n",
    "    ############ utils #########################\n",
    "    \n",
    "    def get_file(self, path):\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            data = json.loads(json.load(f))\n",
    "        return data\n",
    "    \n",
    "    def text_from_json(self, json_file):\n",
    "        all_text = []\n",
    "        for file in json_file:\n",
    "            for sample in file:\n",
    "                text_l = sample['text']\n",
    "                for sentence in text_l:\n",
    "                    sent = sentence.lower()\n",
    "                    all_text.append(sent)\n",
    "        return all_text\n",
    "    \n",
    "\n",
    "    ############## PROCESSING DATA ##############\n",
    "    \n",
    "    def remove_all_letters_in_text_tags_from_alphabet(self, alphabet, positive_tags, all_text):\n",
    "        \"\"\"\n",
    "        Takes in an alphabet, a list of tags and a list of sentences. Returns an alphabet that correspond to the \n",
    "        negative samples by substracting the positive tags\n",
    "        \"\"\"\n",
    "        # 1) Get alphabet cropped to the length of sentences\n",
    "        idx_len = len(all_text)\n",
    "        cropped_alphabet = alphabet[:idx_len]\n",
    "\n",
    "\n",
    "        # 2) If not positive tags, return cropped_alpha\n",
    "        if positive_tags == []:\n",
    "            return cropped_alphabet\n",
    "\n",
    "        # 3) Iterate over positive tags and remove them from cropped_alphabet. \n",
    "        for tag in positive_tags:\n",
    "            new_alphabet = cropped_alphabet.replace(tag, \"\")\n",
    "            cropped_alphabet = new_alphabet\n",
    "\n",
    "        # 4) The result is the negative tags! :)\n",
    "        return new_alphabet\n",
    "\n",
    "        \n",
    "    def get_all_text(self, files):\n",
    "        \"\"\"\n",
    "        Parse json file and outputs train_data (text) and numpy array labels for binary classification\n",
    "        \"\"\"\n",
    "        self.sentences = []\n",
    "        self.sentences_labels = []\n",
    "        \n",
    "        for file in files:\n",
    "            # iterate over the examples in file and grab positive and negative samples\n",
    "            for i in range(len(file)):\n",
    "                # elements from dictionary\n",
    "                positive_tags = file[i]['text-tags']\n",
    "                text_list = file[i]['text']\n",
    "\n",
    "                # valid text\n",
    "                valid_text = [ text_list[ALPHABET.index(letter)].lower() for letter in positive_tags ]\n",
    "\n",
    "                # nonvalid text\n",
    "                negative_tags = self.remove_all_letters_in_text_tags_from_alphabet(ALPHABET, positive_tags, text_list)\n",
    "                nonvalid_text = [ text_list[ALPHABET.index(letter)].lower() for letter in negative_tags ] \n",
    "                \n",
    "                # labels\n",
    "                pos_label = np.array([0,1])\n",
    "                neg_label = np.array([1,0])\n",
    "\n",
    "                # append sentences and labels that are not empty lists\n",
    "                if len(nonvalid_text) != 0:\n",
    "                    \n",
    "                    for nv_text in nonvalid_text:\n",
    "                        self.sentences.append(nv_text)\n",
    "                        self.sentences_labels.append(neg_label)\n",
    "\n",
    "                if len(valid_text) != 0:\n",
    "                    \n",
    "                    for v_text in valid_text:\n",
    "                        self.sentences.append(v_text)\n",
    "                        self.sentences_labels.append(pos_label)\n",
    "\n",
    "        # from list to tensor\n",
    "        self.sentences_labels = np.array(self.sentences_labels)\n",
    "        self.sentences_labels = torch.from_numpy(self.sentences_labels)\n",
    "        \n",
    "    def tokenize_sentences(self):\n",
    "        \n",
    "        # Tokenize all sentences and map the tokens to their word ID \n",
    "        self.inputs_ids = []\n",
    "        self.attention_masks = []\n",
    "        \n",
    "        for sent in self.sentences:                \n",
    "            # `encode_plus` will:\n",
    "            #   (1) Tokenize the sentence.\n",
    "            #   (2) Prepend the `[CLS]` token to the start.\n",
    "            #   (3) Append the `[SEP]` token to the end.\n",
    "            #   (4) Map tokens to their IDs.\n",
    "            #   (5) Pad or truncate the sentence to `max_length`\n",
    "            #   (6) Create attention masks for [PAD] tokens.\n",
    "            encoded_dict = tokenizer.encode_plus(sent,\n",
    "                                                add_special_tokens = True, # add [CLS] and [SEP]\n",
    "                                                max_length = 64,\n",
    "                                                pad_to_max_length = True,  # pad and truncate\n",
    "                                                return_attention_mask = True,\n",
    "                                                return_tensors = 'pt'\n",
    "                                                )\n",
    "            self.inputs_ids.append(encoded_dict['input_ids'])\n",
    "            self.attention_masks.append(encoded_dict['attention_mask'])\n",
    "            \n",
    "#             print('original: ', sent)\n",
    "#             print('token IDs: ', encoded_dict['input_ids'])\n",
    "\n",
    "        # Convert list to tensors\n",
    "        self.inputs_ids = torch.cat(self.inputs_ids, dim=0)\n",
    "        self.attention_masks = torch.cat(self.attention_masks, dim=0)\n",
    "        \n",
    "    def partition_data(self, train_percentage):\n",
    "        \n",
    "        assert len(self.inputs_ids) == len(self.sentences_labels)\n",
    "        dataset = TensorDataset(self.inputs_ids, self.attention_masks, self.sentences_labels)\n",
    "        \n",
    "        train_size = int(train_percentage * len(dataset))\n",
    "        dev_size = len(dataset) - train_size\n",
    "\n",
    "        \n",
    "        self.train_dataset, self.dev_dataset = random_split(dataset, [train_size, dev_size])\n",
    "        \n",
    "        print('{:>5,} training samples'.format(train_size))\n",
    "        print('{:>5,} validation samples'.format(dev_size))\n",
    "\n",
    "    \n",
    "    def RUN_for_dataset(self):\n",
    "        \n",
    "        # 1) get jsons\n",
    "        train_raw = []\n",
    "        for i in range(len(self.train_path)): # list with all training data from different sections\n",
    "            train_raw.append(self.get_file(self.train_path[i]))\n",
    "        \n",
    "        # 2) get text\n",
    "        self.get_all_text(train_raw)\n",
    "        print(len(self.sentences), len(self.sentences_labels))\n",
    "        # 3) tokenize at word-level\n",
    "        self.tokenize_sentences()\n",
    "        \n",
    "        # 4) partition data\n",
    "        self.partition_data(.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9628 9628\n",
      "7,702 training samples\n",
      "1,926 validation samples\n"
     ]
    }
   ],
   "source": [
    "dataset = PreprocessedData_wordlevel([\"./data/architecture_dz-cleaned-tagged.json\",\n",
    "                            \"./data/design_dz-cleaned-tagged.json\",\n",
    "                           \"./data/technology_dz-cleaned-tagged.json\", ], \n",
    "                           [\"./data/architecture_dz-cleaned.json\", \n",
    "                            \"./data/design_dz-cleaned.json\",\n",
    "                           \"./data/technology_dz-cleaned.json\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
