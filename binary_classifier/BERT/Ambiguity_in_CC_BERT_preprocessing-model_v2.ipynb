{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install python-Levenshtein\n",
    "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
    "!pip install wget\n",
    "%cd ctcdecode\n",
    "!pip install .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.6.0-cp37-cp37m-macosx_10_10_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import * # for pad_sequence and whatnot\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\"\"\"\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.models import BPE \n",
    "from tokenizers.normalizers import Lowercase, NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "\"\"\"\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To have more information about what's happening under the hood\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
    "!pip install wget\n",
    "%cd ctcdecode\n",
    "!pip install .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/manuelladron/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "INFO:transformers.configuration_utils:Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/manuelladron/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/manuelladron/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = 'abcdefghijklmnopqrstuvwxyz'\n",
    "NUMBERS = '0123456789'\n",
    "class PreprocessedData_wordlevel(object):\n",
    "    def __init__(self, train_file_path, test_file_path):\n",
    "        \"\"\"\n",
    "        train_file_path = list with files\n",
    "        test_file_path = list with files\n",
    "        \"\"\"\n",
    "        \n",
    "        self.train_path = train_file_path\n",
    "        self.test_path = test_file_path\n",
    "        \n",
    "        self.VOCAB = None\n",
    "        self.VOCAB_SIZE = None\n",
    "                \n",
    "        # Automatically run it when making an instance\n",
    "        self.RUN_for_dataset()\n",
    "\n",
    "    ############ utils #########################\n",
    "    \n",
    "    def get_file(self, path):\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            data = json.loads(json.load(f))\n",
    "        return data\n",
    "    \n",
    "    def text_from_json(self, json_file):\n",
    "        all_text = []\n",
    "        for file in json_file:\n",
    "            for sample in file:\n",
    "                text_l = sample['text']\n",
    "                for sentence in text_l:\n",
    "                    sent = sentence.lower()\n",
    "                    all_text.append(sent)\n",
    "        return all_text\n",
    "    \n",
    "\n",
    "    ############## PROCESSING DATA ##############\n",
    "    \n",
    "    def remove_all_letters_in_text_tags_from_alphabet(self, alphabet, positive_tags, all_text):\n",
    "        \"\"\"\n",
    "        Takes in an alphabet, a list of tags and a list of sentences. Returns an alphabet that correspond to the \n",
    "        negative samples by substracting the positive tags\n",
    "        \"\"\"\n",
    "        # 1) Get alphabet cropped to the length of sentences\n",
    "        idx_len = len(all_text)\n",
    "        cropped_alphabet = alphabet[:idx_len]\n",
    "\n",
    "\n",
    "        # 2) If not positive tags, return cropped_alpha\n",
    "        if positive_tags == []:\n",
    "            return cropped_alphabet\n",
    "\n",
    "        # 3) Iterate over positive tags and remove them from cropped_alphabet. \n",
    "        for tag in positive_tags:\n",
    "            new_alphabet = cropped_alphabet.replace(tag, \"\")\n",
    "            cropped_alphabet = new_alphabet\n",
    "\n",
    "        # 4) The result is the negative tags! :)\n",
    "        return new_alphabet\n",
    "\n",
    "        \n",
    "    def get_all_text(self, files):\n",
    "        \"\"\"\n",
    "        Parse json file and outputs train_data (text) and numpy array labels for binary classification\n",
    "        \"\"\"\n",
    "        self.sentences = []\n",
    "        self.sentences_labels = []\n",
    "        \n",
    "        for file in files:\n",
    "            # iterate over the examples in file and grab positive and negative samples\n",
    "            for i in range(len(file)):\n",
    "                # elements from dictionary\n",
    "                positive_tags = file[i]['text-tags']\n",
    "                text_list = file[i]['text']\n",
    "\n",
    "                # valid text\n",
    "                valid_text = [ text_list[ALPHABET.index(letter)].lower() for letter in positive_tags ]\n",
    "\n",
    "                # nonvalid text\n",
    "                negative_tags = self.remove_all_letters_in_text_tags_from_alphabet(ALPHABET, positive_tags, text_list)\n",
    "                nonvalid_text = [ text_list[ALPHABET.index(letter)].lower() for letter in negative_tags ] \n",
    "                \n",
    "                # labels\n",
    "#                 pos_label = np.array([0,1])\n",
    "#                 neg_label = np.array([1,0])\n",
    "\n",
    "                pos_label = np.array([1])\n",
    "                neg_label = np.array([0])\n",
    "\n",
    "                # append sentences and labels that are not empty lists\n",
    "                if len(nonvalid_text) != 0:\n",
    "                    \n",
    "                    for nv_text in nonvalid_text:\n",
    "                        self.sentences.append(nv_text)\n",
    "                        self.sentences_labels.append(neg_label)\n",
    "\n",
    "                if len(valid_text) != 0:\n",
    "                    \n",
    "                    for v_text in valid_text:\n",
    "                        self.sentences.append(v_text)\n",
    "                        self.sentences_labels.append(pos_label)\n",
    "\n",
    "        # from list to array\n",
    "        self.sentences_labels = np.array(self.sentences_labels, dtype='int64')\n",
    "\n",
    "    def tokenize_sentences(self):\n",
    "        \n",
    "        # Tokenize all sentences and map the tokens to their word ID \n",
    "        self.inputs_ids = []\n",
    "        self.attention_masks = []\n",
    "        \n",
    "        for sent in self.sentences:                \n",
    "            # `encode_plus` will:\n",
    "            #   (1) Tokenize the sentence.\n",
    "            #   (2) Prepend the `[CLS]` token to the start.\n",
    "            #   (3) Append the `[SEP]` token to the end.\n",
    "            #   (4) Map tokens to their IDs.\n",
    "            #   (5) Pad or truncate the sentence to `max_length`\n",
    "            #   (6) Create attention masks for [PAD] tokens.\n",
    "            encoded_dict = tokenizer.encode_plus(sent,\n",
    "                                                add_special_tokens = True, # add [CLS] and [SEP]\n",
    "                                                max_length = 64,\n",
    "                                                pad_to_max_length = True,  # pad and truncate\n",
    "                                                return_attention_mask = True,\n",
    "                                                return_tensors = 'pt'\n",
    "                                                )\n",
    "            self.inputs_ids.append(encoded_dict['input_ids'])\n",
    "            self.attention_masks.append(encoded_dict['attention_mask'])\n",
    "            \n",
    "#             print('original: ', sent)\n",
    "#             print('token IDs: ', encoded_dict['input_ids'])\n",
    "\n",
    "        # Convert list to tensors\n",
    "        self.inputs_ids = torch.cat(self.inputs_ids, dim=0)\n",
    "        self.attention_masks = torch.cat(self.attention_masks, dim=0)\n",
    "        self.labels = torch.tensor(self.sentences_labels)\n",
    "        print(self.labels.shape)\n",
    "        print(self.labels.type())\n",
    "    def partition_data(self, train_percentage):\n",
    "        \n",
    "        assert len(self.inputs_ids) == len(self.sentences_labels)\n",
    "        dataset = TensorDataset(self.inputs_ids, self.attention_masks, self.labels)\n",
    "        \n",
    "        train_size = int(train_percentage * len(dataset))\n",
    "        dev_size = len(dataset) - train_size\n",
    "\n",
    "        \n",
    "        self.train_dataset, self.dev_dataset = random_split(dataset, [train_size, dev_size])\n",
    "        \n",
    "        print('{:>5,} training samples'.format(train_size))\n",
    "        print('{:>5,} validation samples'.format(dev_size))\n",
    "\n",
    "    \n",
    "    def RUN_for_dataset(self):\n",
    "        \n",
    "        # 1) get jsons\n",
    "        train_raw = []\n",
    "        for i in range(len(self.train_path)): # list with all training data from different sections\n",
    "            train_raw.append(self.get_file(self.train_path[i]))\n",
    "        \n",
    "        # 2) get text\n",
    "        self.get_all_text(train_raw)\n",
    "        print(len(self.sentences), len(self.sentences_labels))\n",
    "        # 3) tokenize at word-level\n",
    "        self.tokenize_sentences()\n",
    "        \n",
    "        # 4) partition data\n",
    "        self.partition_data(.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9628 9628\n",
      "torch.Size([9628, 1])\n",
      "torch.LongTensor\n",
      "7,702 training samples\n",
      "1,926 validation samples\n"
     ]
    }
   ],
   "source": [
    "dataset = PreprocessedData_wordlevel([\"./data/architecture_dz-cleaned-tagged.json\",\n",
    "                            \"./data/design_dz-cleaned-tagged.json\",\n",
    "                           \"./data/technology_dz-cleaned-tagged.json\", ], \n",
    "                           [\"./data/architecture_dz-cleaned.json\", \n",
    "                            \"./data/design_dz-cleaned.json\",\n",
    "                           \"./data/technology_dz-cleaned.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.train_dataset\n",
    "dev_dataset = dataset.dev_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(device)\n",
    "num_workers = 8 if cuda else 0 \n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            dev_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(dev_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/manuelladron/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "INFO:transformers.configuration_utils:Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/manuelladron/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training:\n",
    "\n",
    "- Unpack our data inputs and labels\n",
    "- Load data onto the GPU for acceleration\n",
    "- Clear out the gradients calculated in the previous pass.\n",
    "    - In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.\n",
    "- Forward pass (feed input data through the network)\n",
    "- Backward pass (backpropagation)\n",
    "- Tell the network to update parameters with optimizer.step()\n",
    "- Track variables for monitoring progress\n",
    "\n",
    "Evalution:\n",
    "\n",
    "- Unpack our data inputs and labels\n",
    "- Load data onto the GPU for acceleration\n",
    "- Forward pass (feed input data through the network)\n",
    "- Compute loss on our validation data and track variables for monitoring progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(epochs, train, test, train_name, val_name, name_long, name_short):\n",
    "    BCK = (3/255, 5/255, 25/255)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_facecolor(BCK)\n",
    "    ax.plot(epochs, train, 'g', label=train_name, c=\"mediumvioletred\")\n",
    "    ax.plot(epochs, test, 'b', label=val_name, c=\"darkturquoise\")\n",
    "    ax.set_title(name_long)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(name_short)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, scheduler):\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    \n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(loader):\n",
    "        \n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)  # batch x seq_length\n",
    "        b_input_mask = batch[1].to(device) # batch x seq_length\n",
    "        b_labels = batch[2].to(device)     # batch x 2 (num_classes, binary)\n",
    "\n",
    "#         print(\"inputs_ids shape: \", b_input_ids.shape)\n",
    "#         print(\"input_mask shape: \", b_input_mask.shape)\n",
    "#         print(\"labels shape: \", b_labels.shape)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        loss = loss.clone().detach().requires_grad_(True)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        \n",
    "        accuracy = flat_accuracy(logits, b_labels)\n",
    "        running_acc += accuracy\n",
    "        \n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    running_loss /= len(loader)\n",
    "    running_acc /= len(loader)\n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(running_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "    return running_loss, running_acc, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in loader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, # bc of single sentences?\n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        running_acc += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    running_acc /= len(validation_dataloader)\n",
    "    \n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    running_loss /= len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(running_loss))\n",
    "    print(\"  Validation Accuracy: {0:.2f}\".format(running_acc))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    return running_loss, running_acc, validation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs(model, train_dataloader, validation_dataloader, optimizer, scheduler, epochs):\n",
    "    # This training code is based on the `run_glue.py` script here:\n",
    "    # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "    # Set the seed value all over the place to make this reproducible.\n",
    "    seed_val = 42\n",
    "\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    train_losses, train_accs = [], []\n",
    "    test_losses, test_accs = [] , []\n",
    "    epochs_l = []\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        train_loss, train_acc, train_time = train(model, train_dataloader, optimizer, scheduler)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        test_loss, test_acc, test_time = test(model, validation_dataloader)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': train_loss,\n",
    "                'Training Accur.': train_acc,\n",
    "                'Valid. Loss': test_loss,\n",
    "                'Valid. Accur.': test_acc,\n",
    "                'Training Time': train_time,\n",
    "                'Validation Time': test_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Epochs\n",
    "        epochs_l.append(epoch_i)\n",
    "        make_graph(epochs, train_accs, test_accs, 'Training Acc', 'Testing Acc',\n",
    "                       'Training and Testing Accuracy', 'Accuracy')\n",
    "\n",
    "        make_graph(epochs, train_losses, test_losses, 'Training loss', 'Testing loss',\n",
    "                       'Training and Testing loss', 'Loss')\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    return train_losses, train_acss, test_losses, test_acss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    241.    Elapsed: 0:01:11.\n",
      "  Batch    80  of    241.    Elapsed: 0:02:25.\n",
      "  Batch   120  of    241.    Elapsed: 0:03:39.\n",
      "  Batch   160  of    241.    Elapsed: 0:04:54.\n",
      "  Batch   200  of    241.    Elapsed: 0:06:09.\n",
      "  Batch   240  of    241.    Elapsed: 0:07:24.\n",
      "\n",
      "  Average training loss: 0.91\n",
      "  Training epcoh took: 0:07:26\n",
      "  Validation Loss: 0.92\n",
      "  Validation Accuracy: 0.38\n",
      "  Validation took: 0:01:44\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    241.    Elapsed: 0:01:20.\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accs, test_losses, test_accs = run_epochs(model, train_dataloader, validation_dataloader, optimizer, scheduler, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
