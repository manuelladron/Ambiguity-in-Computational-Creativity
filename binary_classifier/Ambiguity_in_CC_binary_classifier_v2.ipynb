{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install python-Levenshtein\n",
    "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
    "!pip install wget\n",
    "%cd ctcdecode\n",
    "!pip install .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import * # for pad_sequence and whatnot\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
    "!pip install wget\n",
    "%cd ctcdecode\n",
    "!pip install .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = 'abcdefghijklmnopqrstuvwxyz'\n",
    "NUMBERS = '0123456789'\n",
    "\n",
    "class PreprocessedData(object):\n",
    "    def __init__(self, train_file_path, test_file_path):\n",
    "        \"\"\"\n",
    "        train_file_path = list with files\n",
    "        test_file_path = list with files\n",
    "        \"\"\"\n",
    "        \n",
    "        self.train_path = train_file_path\n",
    "        self.test_path = test_file_path\n",
    "        \n",
    "        self.VOCAB = None\n",
    "        self.VOCAB_SIZE = None\n",
    "        \n",
    "        # Dictionary to convert char to integers\n",
    "        self.char2index = None\n",
    "        \n",
    "        # Dataset\n",
    "        self.train_data = None \n",
    "        self.dev_data = None\n",
    "        self.train_labels = None\n",
    "        self.dev_labels = None\n",
    "        \n",
    "        # Automatically run it when making an instance\n",
    "        self.RUN_for_vocab()\n",
    "        self.RUN_for_dataset()\n",
    "\n",
    "        \n",
    "    def get_file(self, path):\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            data = json.loads(json.load(f))\n",
    "        return data\n",
    "    \n",
    "    def text_from_json(self, json_file):\n",
    "        all_text = []\n",
    "        for file in json_file:\n",
    "            for sample in file:\n",
    "                text_l = sample['text']\n",
    "                for sentence in text_l:\n",
    "                    sent = sentence.lower()\n",
    "                    all_text.append(sent)\n",
    "        return all_text\n",
    "    \n",
    "    ################# VOCABULARY ##############\n",
    "    \n",
    "    def get_all_chars(self, text_list):\n",
    "        all_chars = []\n",
    "        for sentences in text_list:\n",
    "            for char in sentences:\n",
    "                all_chars.append(char)\n",
    "        chars = sorted(list(set(all_chars)))\n",
    "        return chars\n",
    "    \n",
    "    def get_vocabulary(self, json_files):\n",
    "        \"\"\"\n",
    "        from test json file (includes elements in training), get all unique chars\n",
    "        \"\"\"\n",
    "        text = self.text_from_json(json_files)\n",
    "        chars = self.get_all_chars(text)\n",
    "        return chars\n",
    "    \n",
    "    def word_2_index(self, VOCAB):\n",
    "        char_to_int = dict((c,i) for i,c in enumerate(VOCAB))\n",
    "        self.char2index = char_to_int\n",
    "    \n",
    "    def RUN_for_vocab(self):\n",
    "        # 1) Get json for vocabulary\n",
    "        train_and_test_samples = []\n",
    "        for i in range(len(self.test_path)):\n",
    "            sample = self.get_file(self.test_path[i]) # Again, testpath includes train samples\n",
    "            train_and_test_samples.append(sample)\n",
    "            \n",
    "        # 2) Get vocabulary\n",
    "        self.VOCAB = self.get_vocabulary(train_and_test_samples)\n",
    "        self.VOCAB_SIZE = len(self.VOCAB)\n",
    "        \n",
    "        # 3) Get dictionary\n",
    "        self.word_2_index(self.VOCAB)\n",
    "        \n",
    "    ############## PROCESSING DATA ##############\n",
    "    \n",
    "    def remove_all_letters_in_text_tags_from_alphabet(self, alphabet, positive_tags, all_text):\n",
    "        \"\"\"\n",
    "        Takes in an alphabet, a list of tags and a list of sentences. Returns an alphabet that correspond to the \n",
    "        negative samples by substracting the positive tags\n",
    "        \"\"\"\n",
    "        # 1) Get alphabet cropped to the length of sentences\n",
    "        idx_len = len(all_text)\n",
    "        cropped_alphabet = alphabet[:idx_len]\n",
    "\n",
    "\n",
    "        # 2) If not positive tags, return cropped_alpha\n",
    "        if positive_tags == []:\n",
    "            return cropped_alphabet\n",
    "\n",
    "        # 3) Iterate over positive tags and remove them from cropped_alphabet. \n",
    "        for tag in positive_tags:\n",
    "            new_alphabet = cropped_alphabet.replace(tag, \"\")\n",
    "            cropped_alphabet = new_alphabet\n",
    "\n",
    "        # 4) The result is the negative tags! :)\n",
    "        return new_alphabet\n",
    "\n",
    "        \n",
    "    def get_all_text(self, files):\n",
    "        \"\"\"\n",
    "        Parse json file and outputs train_data (text) and numpy array labels for binary classification\n",
    "        \"\"\"\n",
    "        train_data = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for file in files:\n",
    "            # iterate over the examples in file and grab positive and negative samples\n",
    "            for i in range(len(file)):\n",
    "\n",
    "                # elements from dictionary\n",
    "                positive_tags = file[i]['text-tags']\n",
    "                text_list = file[i]['text']\n",
    "\n",
    "                # valid text\n",
    "                valid_text = [ text_list[ALPHABET.index(letter)].lower() for letter in positive_tags ]\n",
    "\n",
    "                # nonvalid text\n",
    "                negative_tags = self.remove_all_letters_in_text_tags_from_alphabet(ALPHABET, positive_tags, text_list)\n",
    "                nonvalid_text = [ text_list[ALPHABET.index(letter)].lower() for letter in negative_tags ] \n",
    "\n",
    "                # labels\n",
    "                pos_label = np.array([0,1])\n",
    "                neg_label = np.array([1,0])\n",
    "\n",
    "    #             pos_label = np.array([1])\n",
    "    #             neg_label = np.array([0])\n",
    "\n",
    "                # store samples and labels that are not empty lists\n",
    "                if len(nonvalid_text) != 0:\n",
    "                    train_data.append(nonvalid_text)\n",
    "                    train_labels.append(pos_label)\n",
    "\n",
    "                if len(valid_text) != 0:\n",
    "                    train_data.append(valid_text)\n",
    "                    train_labels.append(neg_label)\n",
    "\n",
    "\n",
    "        return train_data, np.array(train_labels)\n",
    "        \n",
    "    def convert_text_to_int_array(self, text, dic):\n",
    "        \"\"\"\n",
    "        Convert text dataset to int array\n",
    "        \"\"\"\n",
    "        all_ints = []\n",
    "        for sample in text: \n",
    "            for sentence in sample:\n",
    "                sent_len = len(sentence)\n",
    "                sent_array = np.zeros(sent_len, dtype = int)\n",
    "                for i, char in enumerate(sentence):\n",
    "                    val = dic[char]\n",
    "                    sent_array[i] = val\n",
    "            all_ints.append(sent_array)\n",
    "        return np.array(all_ints)  \n",
    "    \n",
    "    \n",
    "    def partition_data(self, data_set, label_set, train_percentage):\n",
    "        train_len = int(train_percentage*data_set.size)\n",
    "        dev_len = data_set.size - train_len\n",
    "\n",
    "        # train\n",
    "        train_set = data_set[:train_len]\n",
    "        train_labels = label_set[:train_len]\n",
    "\n",
    "        # development\n",
    "        dev_set = data_set[train_len:]\n",
    "        dev_labels = label_set[train_len:]\n",
    "\n",
    "        return train_set, dev_set, train_labels, dev_labels   \n",
    "    \n",
    "    def RUN_for_dataset(self):\n",
    "        train_raw = []\n",
    "        for i in range(len(self.train_path)): # list with all training data from different sections\n",
    "            train_raw.append(self.get_file(self.train_path[i]))\n",
    "        \n",
    "        raw_dataset, labels_dataset = self.get_all_text(train_raw)\n",
    "        data_set = self.convert_text_to_int_array(raw_dataset, self.char2index)\n",
    "        self.train_data, self.dev_data, self.train_labels, self.dev_labels = self.partition_data(data_set, labels_dataset, .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PreprocessedData([\"./data/architecture_dz-cleaned-tagged.json\",\n",
    "                            \"./data/design_dz-cleaned-tagged.json\",\n",
    "                           \"./data/technology_dz-cleaned-tagged.json\"], \n",
    "                           [\"./data/architecture_dz-cleaned.json\", \n",
    "                            \"./data/design_dz-cleaned.json\",\n",
    "                           \"./data/technology_dz-cleaned.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1782,), (446,))"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train_data.shape, dataset.dev_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextDataset and Collate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chris' code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.length = len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        X = self.data[i]\n",
    "        Y = self.labels[i]\n",
    "        return X, Y\n",
    "\n",
    "def collate(seq_list):\n",
    "    # Get inputs shapes and sequences\n",
    "    x = pad_sequence([torch.tensor(s[0]).to(DEVICE) for s in seq_list])\n",
    "    lengths = torch.LongTensor([len(s[0]) for s in seq_list])\n",
    "\n",
    "    # Assign binary classification\n",
    "\n",
    "    targets = torch.tensor([[s[1][0].item(), s[1][0].item()] for s in seq_list])\n",
    "    \n",
    "    return x, lengths, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "\n",
    "    #define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional, dropout):\n",
    "        super(classifier, self).__init__()\n",
    "\n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        #lstm layer\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "\n",
    "        #dense layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "        #activation function\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "\n",
    "        #text = [batch size, sent_length]\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent_len, emb dim] ---> say [2, 305, 100] : 100 dimensions for each of the 305 characters \n",
    "        \n",
    "        #packed sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, enforce_sorted=False) #, batch_first=True)\n",
    "        #packed_embdded = [XXXX, emb dimension]\n",
    "\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        #hidden = [batch size, num layers * num directions, hid dim]\n",
    "        #cell = [batch size, num layers * num directions,hid dim]\n",
    "\n",
    "        #concat the final forward and backward hidden state\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "\n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        dense_outputs=self.fc(hidden)\n",
    "\n",
    "        #Final activation function\n",
    "        outputs=self.act(dense_outputs)\n",
    "\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL SETUP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(DEVICE)\n",
    "num_workers = 8 if cuda else 0 \n",
    "\n",
    "batch_size_gpu = 64\n",
    "batch_size_cpu = 64\n",
    "\n",
    "size_of_vocab = dataset.VOCAB_SIZE\n",
    "embedding_dim = 100\n",
    "num_hidden_nodes = 32\n",
    "num_output_nodes = 2\n",
    "num_layers = 2\n",
    "bidirection = True\n",
    "dropout = 0.2\n",
    "nepochs = 20\n",
    "lr = 1e-4\n",
    "\n",
    "# Training\n",
    "train_dataset = TextDataset(dataset.train_data, dataset.train_labels)\n",
    "\n",
    "train_loader_args = dict(shuffle=True, batch_size=batch_size_gpu, num_workers=num_workers, pin_memory=True, collate_fn=collate) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=batch_size_cpu, collate_fn = collate)\n",
    "train_loader = data.DataLoader(train_dataset, **train_loader_args)\n",
    "\n",
    "# Development\n",
    "dev_dataset = TextDataset(dataset.dev_data, dataset.dev_labels)\n",
    "\n",
    "dev_loader_args = dict(shuffle=False, batch_size=batch_size_gpu, num_workers=num_workers, pin_memory=True, collate_fn=collate) if cuda\\\n",
    "                    else dict(shuffle=False, collate_fn=collate, batch_size=batch_size_cpu)\n",
    "dev_loader = data.DataLoader(dev_dataset, **dev_loader_args)\n",
    "\n",
    "# Instantiate\n",
    "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes,\n",
    "                   num_layers, bidirectional=bidirection, dropout=dropout)\n",
    "\n",
    "# Criterion & Optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(outs, target):\n",
    "    max_index = outs.max(dim = 1).indices\n",
    "    target_index = target.max(dim=1).indices\n",
    "    num_correct = (max_index == target_index).sum().item()\n",
    "    return num_correct / len(target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(loader, model, criterion, optimizer):\n",
    "    # Place model into mode and onto correct device\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    for (data, lengths, target) in loader:\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Use correct types for data\n",
    "        data = data.to(DEVICE).long()\n",
    "        lengths = lengths.to(DEVICE)\n",
    "        target = target.to(DEVICE).float()\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = model(data, lengths)\n",
    "\n",
    "        # Calculate loss\n",
    "#         print('outputs: ', outputs.shape)\n",
    "#         print(outputs)\n",
    "#         print('target: ', target.shape)\n",
    "#         print(target)\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        accuracy = binary_accuracy(outputs, target)\n",
    "        running_acc += accuracy\n",
    "\n",
    "        # Compute gradients and take step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss /= len(loader)\n",
    "    running_acc /= len(loader)\n",
    "    end_time = time.time()\n",
    "    print('Time: ',end_time - start_time, 's')    \n",
    "    return running_loss, running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lstm(loader, model, criterion):\n",
    "    with torch.no_grad():\n",
    "        # Place into eval mode\n",
    "        model.eval()\n",
    "        model.to(DEVICE)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "\n",
    "        for (data, lengths, target) in loader:\n",
    "            # Use correct types for data\n",
    "            data = data.to(DEVICE).long()\n",
    "            lengths = lengths.to(DEVICE)\n",
    "            target = target.to(DEVICE).float()\n",
    "\n",
    "            # Get model outputs\n",
    "            outputs = model(data, lengths)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, target)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            accuracy = binary_accuracy(outputs, target)\n",
    "            running_acc += accuracy\n",
    "\n",
    "    running_loss /= len(loader)\n",
    "    running_acc /= len(loader)\n",
    "\n",
    "    return running_loss, running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(epochs, train, test, train_name, val_name, name_long, name_short):\n",
    "\n",
    "    plt.plot(epochs, train, 'g', label=train_name, c=\"mediumvioletred\")\n",
    "    plt.plot(epochs, test, 'b', label=val_name, c=\"darkturquoise\")\n",
    "    plt.title(name_long)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(name_short)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs(model, optimizer, loader_t, loader_d, criterion, n_epochs):\n",
    "    train_losses, train_accs = [], []\n",
    "    test_losses, test_accs = [] , []\n",
    "    epochs = []\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        print('----- EPOCH ------- \\n', e)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_lstm_Chris(loader_t, model, criterion, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        # Test\n",
    "        test_loss, test_acc = test_lstm_Chris(loader_d, model, criterion)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        # Epochs\n",
    "        epochs.append(e)\n",
    "        if e % 2 == 0 and e != 0:\n",
    "            \n",
    "            print('Training Loss: ', train_loss)\n",
    "            print('Training Accuracy: ', train_acc)\n",
    "\n",
    "        print(\"Train losses:\\n{}\\nTrain Accs:\\n{}\\nTest losses:\\n{}\\nTest accs:\\n{}\\n\".format(train_losses, \n",
    "                                                                                             train_accs, \n",
    "                                                                                             test_losses, \n",
    "                                                                                             test_accs))\n",
    "        make_graph(epochs, train_accs, test_accs, 'Training Acc', 'Testing Acc',\n",
    "                   'Training and Testing Accuracy', 'Accuracy')\n",
    "        make_graph(epochs, train_losses, test_losses, 'Training loss', 'Testing loss',\n",
    "                   'Training and Testing loss', 'Loss')\n",
    "\n",
    "        # save model\n",
    "        torch.save(model.state_dict(), \"./saved_models/v4_{}.pth\".format(e))\n",
    "    \n",
    "    return train_losses, train_accs, test_losses, test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_load = './saved_models/v3_7.pth'\n",
    "model.load_state_dict(torch.load(path_to_load, map_location=DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- EPOCH ------- \n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accs, test_losses, test_accs = run_epochs(model, optimizer, train_loader, dev_loader, criterion, nepochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
