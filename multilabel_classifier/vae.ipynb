{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "import cv2\n",
    "import PIL  \n",
    "import nltk\n",
    "import json\n",
    "import utils\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import utils\n",
    "import spacy    \n",
    "import itertools\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(69)\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified version https://github.com/sksq96/pytorch-vae/blob/master/vae-cnn.ipynb\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class UnFlatten(nn.Module):\n",
    "    def __init__(self, h):\n",
    "        super(UnFlatten, self).__init__() \n",
    "        self.h = h\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), self.h, 1, 1)\n",
    "    \n",
    "class VAE_CNN(nn.Module):\n",
    "    def __init__(self, image_channels=1, h_dim=1024, z_dim=32):\n",
    "        super(VAE_CNN, self).__init__()\n",
    "        self.h = h_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 128, kernel_size=4, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=4, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=4, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(self.h),\n",
    "            nn.ConvTranspose2d(h_dim, 512, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, image_channels, kernel_size=6, stride=2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "#         print('z_shape', z.shape)\n",
    "#         pdb.set_trace()\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/ Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x.view(-1, 4096), x.view(-1, 4096), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train(epoch, log_interval):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "    \n",
    "    return train_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(epoch, batch_size):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    loader = test_loader\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(loader):\n",
    "            data = data.to(device)\n",
    "            pdb.set_trace()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            n = min(data.size(0), 8)\n",
    "            new_recon_batch = recon_batch.view(batch_size, 1, 64, 64)[:n]\n",
    "            pdb.set_trace()\n",
    "            comparison = torch.cat([data[:n], new_recon_batch])\n",
    "            save_image(comparison.cpu(),\n",
    "                       './results_new/%d_recon_' % i + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Running the ecoder through random images from each of the words in the interpolation words list, to find the latent space for these words given our model. Then, we linearly interpolate between those latent spaces to create an animation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_random_image(word):\n",
    "    image_dir = './image_folder_data/%s/' % word\n",
    "    images = os.listdir(image_dir)\n",
    "    sample_image = None\n",
    "#     np.random.shuffle(images)\n",
    "    for image in images:\n",
    "        if '.jpg' in image:\n",
    "            sample_image = image\n",
    "            break\n",
    "            \n",
    "#     if word == 'organic': sample_image = 'd25cd271d208394895d1ad9cb4e7faf5d2c9dcc9.jpg'\n",
    "#     elif word == 'concrete': sample_image = 'f8b6e5a62e4523731d955a72f212dec1f8772b54.jpg'\n",
    "        \n",
    "    sample_image = cv2.imread(image_dir + sample_image, 0)\n",
    "    cv2.imwrite('./interp_experiment/image%s.jpg' % word, sample_image)\n",
    "    \n",
    "    edges = cv2.Canny(sample_image, 0, 255)\n",
    "    sample_image = (sample_image-sample_image.min())/(sample_image.max()-sample_image.min())\n",
    "    \n",
    "    \n",
    "    sample_image = torch.tensor(sample_image).unsqueeze(0).unsqueeze(0)\n",
    "    return sample_image.to(device).float()\n",
    "\n",
    "def getInterpolation(latent_spaces, N): # N is the number of vectors between the spaces\n",
    "    v1 = latent_spaces[0].squeeze()\n",
    "    v2 = latent_spaces[1].squeeze()\n",
    "    vs = []\n",
    "    for i in range(N+1):\n",
    "        cur_weight = i / N\n",
    "        vs.append(torch.lerp(v1, v2, cur_weight).unsqueeze(0))\n",
    "    return vs\n",
    "\n",
    "def writeImages(images, video_name):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    fps = 24\n",
    "    video = cv2.VideoWriter(video_name, fourcc, fps, (64, 64))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        save_image(image, './interp_experiment/image.jpg')\n",
    "        video.write(cv2.imread(\"./interp_experiment/image.jpg\"))\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    \n",
    "def get_average_z(word):\n",
    "    image_dir = './image_folder_data/%s/' % word\n",
    "    images = os.listdir(image_dir)\n",
    "    cur_z = None\n",
    "    i = 0\n",
    "    for image in images:\n",
    "        if '.jpg' in image:\n",
    "            sample_image = image\n",
    "            \n",
    "            sample_image = cv2.imread(image_dir + sample_image, 0)\n",
    "\n",
    "            edges = cv2.Canny(sample_image, 0, 255)\n",
    "            sample_image = (sample_image-sample_image.min())/(sample_image.max()-sample_image.min())\n",
    "\n",
    "            sample_image = torch.tensor(sample_image).unsqueeze(0).unsqueeze(0)\n",
    "            sample_image = sample_image.to(device).float()\n",
    "            if cur_z == None:\n",
    "                cur_z, mu, logvar = model.encode(sample_image)\n",
    "            else:\n",
    "                z, mu, logvar = model.encode(sample_image)\n",
    "                cur_z += z\n",
    "                \n",
    "            i += 1\n",
    "    cur_z /= i\n",
    "    return cur_z\n",
    "    \n",
    "\n",
    "def interpolation_experiment(interpolation_words, t=1):\n",
    "    model.eval()\n",
    "    video_name = './interp_experiment/from_'\n",
    "    with torch.no_grad():\n",
    "        ### Run the encoder so we get a list of latent spaces\n",
    "        latent_spaces = []\n",
    "        for word in interpolation_words:\n",
    "            if t == 1:\n",
    "                sample_image = get_random_image(word)\n",
    "                z, mu, logvar = model.encode(sample_image)\n",
    "            else:\n",
    "                z = get_average_z(word)\n",
    "            latent_spaces.append(z)\n",
    "            video_name += '%s_to_' % word\n",
    "        video_name += '.mp4'\n",
    "        \n",
    "        ### Linearly interpolate between the images\n",
    "        images = []\n",
    "        interpolated_latent_spaces = getInterpolation(latent_spaces, 256)\n",
    "        for new_z in interpolated_latent_spaces:\n",
    "            decoded_image = model.decode(new_z).cpu().detach()\n",
    "            images.append(decoded_image)\n",
    "            \n",
    "        ### Write a video given the images\n",
    "        writeImages(images, video_name)\n",
    "        \n",
    "    print('Video complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Video complete!\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "batch_size = 1\n",
    "epochs = 50\n",
    "log_interval = 100\n",
    "h = 2048\n",
    "z = 256\n",
    "\n",
    "# Cuda\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print('Device: ', device)\n",
    "\n",
    "# Data\n",
    "kwargs = {'num_workers': 8} if cuda else {}\n",
    "\n",
    "# https://stackoverflow.com/questions/52439364/how-to-convert-rgb-images-to-grayscale-in-pytorch-dataloader\n",
    "trainTransform  = transforms.Compose([transforms.Grayscale(num_output_channels=1), transforms.ToTensor()])\n",
    "\n",
    "# https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder\n",
    "train_percentage = .8\n",
    "dataset = datasets.ImageFolder('./image_folder_data/', transform=trainTransform)\n",
    "\n",
    "train_num = int(len(dataset) * train_percentage)\n",
    "test_num = len(dataset) - train_num\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_num, test_num])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "# Model\n",
    "model = VAE_CNN(1, h, z).to(device)\n",
    "model.load_state_dict(torch.load('./pretrained_models/model%d.pt' % 45))\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "interpolation_experiment(['organic', 'concrete'], t=1)\n",
    "# test(0, 32)\n",
    "\n",
    "# trains = []\n",
    "# tests = []\n",
    "# print('Running Epochs...')\n",
    "# for epoch in range(20, epochs + 1):\n",
    "# #     tr = train(epoch, log_interval)\n",
    "#     te = test(epoch, batch_size)\n",
    "#     with torch.no_grad():\n",
    "#         sample = torch.randn(64, z).to(device)\n",
    "#         sample = model.decode(sample).cpu()\n",
    "#         save_image(sample.view(64, 1, 64, 64),\n",
    "#                    './results/sample_' + str(epoch) + '.png')\n",
    "#     trains.append(tr)\n",
    "#     tests.append(te)\n",
    "#     new_list = [trains, tests]\n",
    "#     with open('losses.pkl', 'wb') as b:\n",
    "#         pickle.dump(new_list, b)\n",
    "    \n",
    "#     torch.save(model.state_dict(), './pretrained_models/model%d.pt' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
