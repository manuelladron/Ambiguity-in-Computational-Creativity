{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import nltk\n",
    "import json\n",
    "import utils\n",
    "import pickle\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import utils\n",
    "import spacy    \n",
    "import itertools\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self, nlp):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "    \n",
    "    \n",
    "    def get_adj_from_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Parses sentence and get a list of cleaned lemmatized adj longer than 1 character. No duplicates\n",
    "        \"\"\"\n",
    "        doc = self.nlp(sentence)\n",
    "        adj = []\n",
    "        for token in doc:\n",
    "            if token.pos_ == 'ADJ':\n",
    "                adj_ = token.lemma_.lower()\n",
    "                if len(adj_) > 1:\n",
    "                    adj.append(adj_)\n",
    "        return list(set(adj))\n",
    "        \n",
    "    def get_adj_from_all_sentences(self, sentences):\n",
    "        \"\"\"\n",
    "        Calls the function get_adj_from_sentence a sentences number of times\n",
    "        \"\"\"\n",
    "        adj = []\n",
    "        for sentence in sentences:\n",
    "            sent_adj = self.get_adj_from_sentence(sentence)\n",
    "            adj.extend(sent_adj)\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(file_path):\n",
    "    \n",
    "    # open file\n",
    "    f = open(file_path, 'rb')\n",
    "    \n",
    "    # dump info to that file\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "    # close file\n",
    "    f.close()\n",
    "    \n",
    "    # return vocab\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = 'abcdefghijklmnopqrstuvwxyz'\n",
    "NUMBERS = '0123456789'\n",
    "\n",
    "class PreprocessedData(object):\n",
    "    def __init__(self, files_paths, images_paths, new_folder, nlp, vocab):\n",
    "        \"\"\"\n",
    "        train_file_path = list with files\n",
    "        test_file_path = list with files\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create pipe\n",
    "        self.nlp = nlp\n",
    "        \n",
    "        # Inherit vocabulary\n",
    "        self.VOCAB = vocab\n",
    "        \n",
    "        # Call to utils\n",
    "        self.Utils = utils.Utils()\n",
    "        \n",
    "        # paths to jsons\n",
    "        self.files_paths = files_paths\n",
    "        \n",
    "        # path to image folders\n",
    "        self.images_paths = images_paths\n",
    "\n",
    "        # path to new image_path\n",
    "        self.new_image_path = new_folder\n",
    "        \n",
    "        # Dataset\n",
    "        self.train_data = None \n",
    "        self.dev_data = None\n",
    "        self.train_labels = None\n",
    "        self.dev_labels = None\n",
    "        \n",
    "        # Run\n",
    "        print(\"\\nRunner....\")\n",
    "        self.runner()\n",
    "        \n",
    "    ############## PROCESSING DATA ##############\n",
    "    def get_adj_from_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Parses sentence and get a list of cleaned lemmatized adj longer than 1 character. No duplicates\n",
    "        \"\"\"\n",
    "        doc = self.nlp(sentence)\n",
    "        adj = []\n",
    "        for token in doc:\n",
    "            if token.pos_ == 'ADJ':\n",
    "                adj_ = token.lemma_.lower()\n",
    "                if len(adj_) > 1:\n",
    "                    adj.append(adj_)\n",
    "        return list(set(adj))\n",
    "        \n",
    "    def get_adj_from_all_sentences(self, sentences):\n",
    "        \"\"\"\n",
    "        Calls the function get_adj_from_sentence a sentences number of times\n",
    "        \"\"\"\n",
    "        adj = []\n",
    "        for sentence in sentences:\n",
    "            sent_adj = self.get_adj_from_sentence(sentence)\n",
    "            adj.extend(sent_adj)\n",
    "        return adj\n",
    "        \n",
    "    def is_valid(self, images, adj):\n",
    "        \"\"\"\n",
    "        Avoids empty data samples\n",
    "        \"\"\"\n",
    "        if adj == []: return False\n",
    "        if images == []: return False\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def handle_N_labels(self, list_of_labels, N_labels):\n",
    "        \"\"\"\n",
    "        Truncates or pads list of labels according to N_labels\n",
    "        \"\"\"\n",
    "        pad_with = list_of_labels[0]\n",
    "        if len(list_of_labels) == N_labels:\n",
    "            return list_of_labels\n",
    "        \n",
    "        elif len(list_of_labels) < N_labels:\n",
    "            diff = N_labels - len(list_of_labels)\n",
    "            for i in range(diff):\n",
    "                list_of_labels.append(pad_with)\n",
    "        else:\n",
    "            list_of_labels = list_of_labels[:N_labels]\n",
    "        \n",
    "        return list_of_labels\n",
    "    \n",
    "    def get_images_labels(self, files):\n",
    "        \"\"\"\n",
    "        Parse json files and outputs train_data (image) + numpy array labels for multi-label classification\n",
    "        \"\"\"\n",
    "        train_data = []\n",
    "        labels = dict()\n",
    "        \n",
    "        for f, file in enumerate(files): # 0_arch 1_des 2_tech\n",
    "            im_per_section = []     # these ones have the same length\n",
    "            for i in range(len(file)): # 902 samples architecture, 675 samples design  \n",
    "                sample_dict = file[i]                  # dictionary\n",
    "                sample_text = sample_dict['text']      # list with sentences (strings)\n",
    "                text_tags = sample_dict['text-tags']\n",
    "                sample_images = sample_dict['images']\n",
    "                image_tags = sample_dict['image-tags']\n",
    "                  \n",
    "                # 2) Select valid text\n",
    "                tagged_text = [sample_text[ALPHABET.index(letter)].lower() for letter in text_tags]\n",
    "                \n",
    "                # 3) Get adjectives from valid text\n",
    "                adj = self.get_adj_from_all_sentences(tagged_text)\n",
    "                \n",
    "                # 4) Select valid images\n",
    "                tagged_images = [sample_images[int(letter)] for letter in image_tags]\n",
    "                \n",
    "                if self.is_valid(tagged_images, adj):\n",
    "                    # Add special token '<end>' as a label\n",
    "#                     adj.append('<end>')\n",
    "#                     adj = self.handle_N_labels(adj, 3)\n",
    "                    for image in tagged_images:\n",
    "                        labels[image[5:]] = adj\n",
    "                    im_per_section.append(tagged_images)\n",
    "            \n",
    "            train_data.append(im_per_section)\n",
    "\n",
    "        return train_data, labels\n",
    "                    \n",
    "    def convert_labels_to_int(self, labels):\n",
    "        \"\"\"\n",
    "        Convert labels to int array\n",
    "        \"\"\"\n",
    "        self.labels_int = dict()\n",
    "        for key, val in labels.items():\n",
    "            label_array = np.zeros(len(val), dtype = int)\n",
    "            for i, label in enumerate(val):\n",
    "                # Try add label in vocabulary. If already exists, nothing happens, just get the idx\n",
    "                self.VOCAB.add_word(label)\n",
    "                idx = self.VOCAB.word2idx[label]\n",
    "                label_array[i] = idx\n",
    "            self.labels_int[key] = label_array\n",
    "        \n",
    "        return self.labels_int\n",
    "    \n",
    "    def copy_wrapper(self, list_dataset_per_section, curr_folders, dest_folder):\n",
    "        self.all_failed_samples = []\n",
    "        for i, dataset in enumerate(list_dataset_per_section):\n",
    "            self.all_failed_samples.append(self.copy_dataset(dataset, curr_folders[i], dest_folder))\n",
    "        \n",
    "    def copy_dataset(self, image_dataset, curr_folder, dest_folder):\n",
    "        fail_samples = self.Utils.copy_files(image_dataset, curr_folder, dest_folder)\n",
    "        return fail_samples\n",
    "    \n",
    "    def flatten(self, S):\n",
    "        if S == []:\n",
    "            return S\n",
    "        if isinstance(S[0], list):\n",
    "            return self.flatten(S[0]) + self.flatten(S[1:])\n",
    "        return S[:1] + self.flatten(S[1:])\n",
    "    \n",
    "    def change_name(self, dataset):\n",
    "        dataset = self.flatten(dataset)\n",
    "        for i, image in enumerate(dataset):\n",
    "            newname = image[5:]\n",
    "            dataset[i] = newname\n",
    "        return dataset\n",
    "            \n",
    "    def remove_dups(self):\n",
    "        seen = []\n",
    "        dups = []\n",
    "        for sample in self.train_data:\n",
    "            if sample not in seen:\n",
    "                seen.append(sample)\n",
    "            else:\n",
    "                dups.append(sample)\n",
    "        \n",
    "        for dup in dups:\n",
    "            if dup in self.train_data:\n",
    "                self.train_data.remove(dup)\n",
    "    \n",
    "    def partition_data(self, data_set, label_set, train_percentage):\n",
    "        train_len = int(train_percentage*data_set.size)\n",
    "        dev_len = data_set.size - train_len\n",
    "\n",
    "        # train\n",
    "        train_set = data_set[:train_len]\n",
    "        train_labels = label_set[:train_len]\n",
    "\n",
    "        # development\n",
    "        dev_set = data_set[train_len:]\n",
    "        dev_labels = label_set[train_len:]\n",
    "\n",
    "    def runner(self):\n",
    "        files = self.Utils.jsons_to_list(self.files_paths)    \n",
    "        self.train_data, self.labels = self.get_images_labels(files) # keep track of the length of these variables\n",
    "        \n",
    "        # This puts all the images in one folder. \n",
    "        self.copy_wrapper(self.train_data, self.images_paths, self.new_image_path)\n",
    "        \n",
    "        # Change names of self.train\n",
    "        self.train_data = self.change_name(self.train_data)\n",
    "        \n",
    "        # This needs to be done after flattening\n",
    "        self.convert_labels_to_int(self.labels)\n",
    "        \n",
    "        # Removes potential duplicates\n",
    "        self.remove_dups()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeImageFolderCompatableData(dataset):\n",
    "    pdb.set_trace()\n",
    "\n",
    "def main():\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "    im_path_arch = \"./dataset/arch_thumbs/big\"\n",
    "    im_path_des = \"./dataset/design_thumbs/big\"\n",
    "    im_path_tech = \"./dataset/tech_thumbs/big\"\n",
    "    im_paths = [im_path_arch, im_path_des, im_path_tech]\n",
    "\n",
    "    tagged_path_arch = \"./dataset/json_files/architecture_dz-cleaned-tagged.json\"\n",
    "    tagged_path_des = \"./dataset/json_files/design_dz-cleaned-tagged.json\"\n",
    "    tagged_path_tech = \"./dataset/json_files/technology_dz-cleaned-tagged.json\"\n",
    "    tagged_files_paths = [tagged_path_arch, tagged_path_des, tagged_path_tech]\n",
    "\n",
    "    vocab_dict_path = \"./dataset/json_files/vocab-dict.json\"\n",
    "    VOCAB = load_vocab(vocab_dict_path)\n",
    "\n",
    "    dataset_new_folder = \"./dataset/al_tagged_images\"\n",
    "    dataset = PreprocessedData(tagged_files_paths, im_paths, dataset_new_folder, nlp, VOCAB)\n",
    "    makeImageFolderCompatableData(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "        self.image_labels_string = dataset.labels    # dictionary\n",
    "        self.image_labels_int = dataset.labels_int\n",
    "        self.image_names = dataset.train_data  # list with image_names\n",
    "        self.name2idx()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.idx2name[index]\n",
    "        name = dataset_new_folder + \"/\" + self.image_names[index]\n",
    "        img = Image.open(name)\n",
    "        \n",
    "        img = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "#         transforms.RandomResizedCrop(100),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()])(img)\n",
    "        \n",
    "        label = torch.from_numpy(self.image_labels_int[image_name])\n",
    "        return img, label\n",
    "    \n",
    "    def name2idx(self):\n",
    "        self.name2idx = dict()\n",
    "        self.idx2name = dict()\n",
    "        for i, key in enumerate(self.image_labels_string.keys()):\n",
    "            self.name2idx[key] = i\n",
    "            self.idx2name[i] = key\n",
    "            \n",
    "def collate(sequence):\n",
    "    \"\"\"\n",
    "    \"the input of this function is the output of function __getitem__\"\n",
    "    \"this gets BATCH_SIZE times GETITEM! \"\n",
    "    if batch_Size == 2 --> sequence is a list with length 2. \n",
    "    Each list is a tuple (image, label) = ((3,64,64), label_length)\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"\\nCollate function....\")\n",
    "    print(\"Sequence: \")\n",
    "    print(len(sequence))\n",
    "    print(\"seq[0] = \", sequence[0][0].shape, sequence[0][1].shape)\n",
    "    print(\"\")\n",
    "    print(\"Seq[1] = \", sequence[1][0].shape, sequence[1][1].shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Concatenate all images in the batch\n",
    "    inputs = torch.cat(([  batch_[0].view(-1, 3, 64, 64) for batch_ in sequence]), dim=0)\n",
    "    \n",
    "    # Pad labels with max_sequence_label\n",
    "    targets  = pad_sequence([batch_[1] for batch_ in sequence], batch_first=True)\n",
    "    targets_length  = torch.LongTensor([len(batch_[1]) for batch_ in sequence])     \n",
    "    \n",
    "#     print(\"\\nInputs: {}\\nTargets: {}\\nTargets length:{}\\n\".format(len(inputs), targets.shape, targets_length))\n",
    "    return inputs, targets, targets_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_processed = ImageDataset(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
